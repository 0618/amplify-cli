"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
var Code = "\nimport base64\nimport datetime\nimport json\nimport logging\nimport os\nimport time\nimport traceback\nimport urllib\nimport urlparse\n\nfrom botocore.auth import SigV4Auth\nfrom botocore.awsrequest import AWSRequest\nfrom botocore.credentials import get_credentials\nfrom botocore.endpoint import BotocoreHTTPSession\nfrom botocore.session import Session\nfrom boto3.dynamodb.types import TypeDeserializer\n\n\n# The following parameters are required to configure the ES cluster\nES_ENDPOINT = os.environ['ES_ENDPOINT']\nES_REGION = os.environ['ES_REGION']\nDEBUG = True if os.environ['DEBUG'] is not None else False\n\n# The following parameters can be optionally customized\nDOC_TABLE_FORMAT = '{}'         # Python formatter to generate index name from the DynamoDB table name\nDOC_TYPE_FORMAT = '{}_type'     # Python formatter to generate type name from the DynamoDB table name, default is to add '_type' suffix\nES_MAX_RETRIES = 3              # Max number of retries for exponential backoff\n\nprint \"Streaming to ElasticSearch\"\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG if DEBUG else logging.INFO)\n\n\n# Subclass of boto's TypeDeserializer for DynamoDB to adjust for DynamoDB Stream format.\nclass StreamTypeDeserializer(TypeDeserializer):\n   def _deserialize_n(self, value):\n       return float(value)\n\n   def _deserialize_b(self, value):\n       return value  # Already in Base64\n\n\nclass ES_Exception(Exception):\n   '''Exception capturing status_code from Client Request'''\n   status_code = 0\n   payload = ''\n\n   def __init__(self, status_code, payload):\n       self.status_code = status_code\n       self.payload = payload\n       Exception.__init__(self, 'ES_Exception: status_code={}, payload={}'.format(status_code, payload))\n\n\n# Low-level POST data to Amazon Elasticsearch Service generating a Sigv4 signed request\ndef post_data_to_es(payload, region, creds, host, path, method='POST', proto='https://'):\n   '''Post data to ES endpoint with SigV4 signed http headers'''\n   req = AWSRequest(method=method, url=proto + host + urllib.quote(path), data=payload, headers={'Host': host})\n   SigV4Auth(creds, 'es', region).add_auth(req)\n   http_session = BotocoreHTTPSession()\n   res = http_session.send(req.prepare())\n   if res.status_code >= 200 and res.status_code <= 299:\n       return res._content\n   else:\n       raise ES_Exception(res.status_code, res._content)\n\n\n# High-level POST data to Amazon Elasticsearch Service with exponential backoff\n# according to suggested algorithm: http://docs.aws.amazon.com/general/latest/gr/api-retries.html\ndef post_to_es(payload):\n   '''Post data to ES cluster with exponential backoff'''\n\n   # Get aws_region and credentials to post signed URL to ES\n   es_region = ES_REGION or os.environ['AWS_REGION']\n   session = Session({'region': es_region})\n   creds = get_credentials(session)\n   es_url = urlparse.urlparse(ES_ENDPOINT)\n   es_endpoint = es_url.netloc or es_url.path  # Extract the domain name in ES_ENDPOINT\n\n   # Post data with exponential backoff\n   retries = 0\n   while retries < ES_MAX_RETRIES:\n       if retries > 0:\n           seconds = (2 ** retries) * .1\n           logger.debug('Waiting for %.1f seconds', seconds)\n           time.sleep(seconds)\n\n       try:\n           es_ret_str = post_data_to_es(payload, es_region, creds, es_endpoint, '/_bulk')\n           logger.debug('Return from ES: %s', es_ret_str)\n           es_ret = json.loads(es_ret_str)\n\n           if es_ret['errors']:\n               logger.error('ES post unsuccessful, errors present, took=%sms', es_ret['took'])\n               # Filter errors\n               es_errors = [item for item in es_ret['items'] if item.get('index').get('error')]\n               logger.error('List of items with errors: %s', json.dumps(es_errors))\n           else:\n               logger.info('ES post successful, took=%sms', es_ret['took'])\n           break  # Sending to ES was ok, break retry loop\n       except ES_Exception as e:\n           if (e.status_code >= 500) and (e.status_code <= 599):\n               retries += 1  # Candidate for retry\n           else:\n               raise  # Stop retrying, re-raise exception\n\n\n# Extracts the DynamoDB table from an ARN\n# ex: arn:aws:dynamodb:eu-west-1:123456789012:table/table-name/stream/2015-11-13T09:23:17.104 should return 'table-name'\ndef get_table_name_from_arn(arn):\n   return arn.split(':')[5].split('/')[1]\n\n\n# Compute a compound doc index from the key(s) of the object in lexicographic order: \"k1=key_val1|k2=key_val2\"\ndef compute_doc_index(keys_raw, deserializer):\n   index = []\n   for key in sorted(keys_raw):\n       index.append('{}={}'.format(key, deserializer.deserialize(keys_raw[key])))\n   return '|'.join(index)\n\n\ndef _lambda_handler(event, context):\n   logger.debug('Event: %s', event)\n   records = event['Records']\n   now = datetime.datetime.utcnow()\n\n   ddb_deserializer = StreamTypeDeserializer()\n   es_actions = []  # Items to be added/updated/removed from ES - for bulk API\n   cnt_insert = cnt_modify = cnt_remove = 0\n\n   for record in records:\n       # Handle both native DynamoDB Streams or Streams data from Kinesis (for manual replay)\n       logger.debug('Record: %s', record)\n       if record.get('eventSource') == 'aws:dynamodb':\n           ddb = record['dynamodb']\n           ddb_table_name = get_table_name_from_arn(record['eventSourceARN'])\n           doc_seq = ddb['SequenceNumber']\n       elif record.get('eventSource') == 'aws:kinesis':\n           ddb = json.loads(base64.b64decode(record['kinesis']['data']))\n           ddb_table_name = ddb['SourceTable']\n           doc_seq = record['kinesis']['sequenceNumber']\n       else:\n           logger.error('Ignoring non-DynamoDB event sources: %s', record.get('eventSource'))\n           continue\n\n       # Compute DynamoDB table, type and index for item\n       doc_table = DOC_TABLE_FORMAT.format(ddb_table_name.lower())  # Use formatter\n       doc_type = DOC_TYPE_FORMAT.format(ddb_table_name.lower())    # Use formatter\n       doc_index = compute_doc_index(ddb['Keys'], ddb_deserializer)\n\n       # Dispatch according to event TYPE\n       event_name = record['eventName'].upper()  # INSERT, MODIFY, REMOVE\n       logger.debug('doc_table=%s, event_name=%s, seq=%s', doc_table, event_name, doc_seq)\n       logger.debug('doc_index=%s', doc_index)\n\n       # Treat events from a Kinesis stream as INSERTs\n       if event_name == 'AWS:KINESIS:RECORD':\n           event_name = 'INSERT'\n\n       # Update counters\n       if event_name == 'INSERT':\n           cnt_insert += 1\n       elif event_name == 'MODIFY':\n           cnt_modify += 1\n       elif event_name == 'REMOVE':\n           cnt_remove += 1\n       else:\n           logger.warning('Unsupported event_name: %s', event_name)\n\n       # If DynamoDB INSERT or MODIFY, send 'index' to ES\n       if (event_name == 'INSERT') or (event_name == 'MODIFY'):\n           if 'NewImage' not in ddb:\n               logger.warning('Cannot process stream if it does not contain NewImage')\n               continue\n           logger.debug('NewImage: %s', ddb['NewImage'])\n           # Deserialize DynamoDB type to Python types\n           doc_fields = ddb_deserializer.deserialize({'M': ddb['NewImage']})\n           # Add metadata\n           doc_fields['@timestamp'] = now.isoformat()\n           doc_fields['@SequenceNumber'] = doc_seq\n\n           # message_ts to int\n           message_ts_int = int(doc_fields['message_ts'])\n           doc_fields['message_ts'] = message_ts_int\n\n           # seenEpoch to ms\n           seenEpochM = int(doc_fields['seenEpoch']*1000)\n           doc_fields['seenEpoch'] = seenEpochM\n\n           # lat_lng\n           lat_lng = str(doc_fields['lat']) + ',' + str(doc_fields['lng'])\n           doc_fields['lat_lng'] = str(lat_lng)\n\n           # add message_latency\n           # starttime = doc_fields['message_ts']\n           # endtime = doc_fields['seenEpoch']\n           # doc_fields['message_latency'] = (endtime - starttime).total_seconds()\n\n           logger.debug('doc_fields: %s', doc_fields)\n\n           # Generate JSON payload\n           doc_json = json.dumps(doc_fields)\n           logger.debug('doc_json: %s', doc_json)\n\n           # Generate ES payload for item\n           action = {'index': {'_index': doc_table, '_type': doc_type, '_id': doc_index}}\n           es_actions.append(json.dumps(action))  # Action line with 'index' directive\n           es_actions.append(doc_json)            # Payload line\n\n       # If DynamoDB REMOVE, send 'delete' to ES\n       elif event_name == 'REMOVE':\n           action = {'delete': {'_index': doc_table, '_type': doc_type, '_id': doc_index}}\n           es_actions.append(json.dumps(action))\n\n   # Prepare bulk payload\n   es_actions.append('')  # Add one empty line to force final \n\n   es_payload = '\n'.join(es_actions)\n   logger.debug('Payload: %s', es_payload)\n\n   logger.info('Posting to ES: inserts=%s updates=%s deletes=%s, total_lines=%s, bytes_total=%s',\n               cnt_insert, cnt_modify, cnt_remove, len(es_actions) - 1, len(es_payload))\n\n   post_to_es(es_payload)  # Post to ES with exponential backoff\n\n\n# Global lambda handler - catches all exceptions to avoid dead letter in the DynamoDB Stream\ndef lambda_handler(event, context):\n   try:\n       return _lambda_handler(event, context)\n   except Exception:\n       logger.error(traceback.format_exc())\n";
exports.default = Code;
//# sourceMappingURL=python_streaming_code.js.map